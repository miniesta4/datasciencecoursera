---
title: "PML Project: Qualitative Activity Recognition"
author: "miniesta4"
date: "February 26, 2020"
output: html_document
---

```{r setup, include=FALSE}
# v202003041749

knitr::opts_chunk$set(echo = FALSE)

library(caret)
library(parallel)
library(doParallel)
library(rattle)

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

set.seed(20200226)
```

## Introduction

People regularly quantify how much of a particular activity is done, but they 
rarely quantify how well they do it. The goal of this project is to predict
the manner in which six participants do the exercise named "Unilateral Dumbbell 
Biceps Curl" using data from accelerometers on the belt, forearm, arm and dumbbell.

Five different classes have been set. Only the A class is the correct manner:

* Class A: Exactly according to the specification
* Class B: Throwing the elbows to the front
* Class C: Lifting the dumbbell only halfway
* Class D: Lowering the dumbbell only halfway
* Class E: Throwing the hips to the front

The data for this project come from this source:  
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

## Exploratory data analysis and pre-processing

Dimensions of the provided data set:

```{r ds}
ds <- read.csv("./fics/pml-training.csv", na.strings = c("", "NA", "#DIV/0!"), 
               stringsAsFactors = FALSE)
dim(ds)

```

The data set is splitted into training and test set in order to estimate the 
test error rate under the validation set approach.

Traininng data set:

```{r in}
inTrain <- createDataPartition(ds$classe, p = 0.75, list = FALSE)
training <- ds[inTrain, ]
testing <- ds[-inTrain, ]
dim(training)
table(training$classe)
```

First seven columns corresponds to variables which are not related to the way the
exercise is performed. They are removed.

```{r training}
training[1:5, 1:7]
training <- training[-c(1:7)]
```

Variables with a high proportion of NA's are removed.

```{r pcj}
pcj_nas <- sapply(training, function(x) mean(is.na(x)))
summary(pcj_nas)
table(pcj_nas < 0.1)
hist(pcj_nas)
index_no_nas <- which(pcj_nas < 0.1)
training <- training[index_no_nas]
```

There are not variables with near zero variability.

```{r nzvp}
nzvp <- nearZeroVar(training, saveMetrics = TRUE)
nzvp[nzvp$nzv,]
```

Final variables in the training set:
```{r names}
dim(training)
names(training)
table(training$classe)
```

The correlation matrix shows that some variables are highly correlated. This 
situation is known as "Collinearity". It can pose problems in the regression context.

```{r M}
M <- abs(cor(subset(training, select = -classe)))
diag(M) <- 0
hist(M, main = "Training: Correlation among vars selected")
```

Variables with correlation > 0.9:

```{r t}
t <- which(M > 0.9, arr.ind = T)
t
```


## Model selection

Collinearity advises against the use of regression models.
Tree-based methods are simple and useful for interpretation.
A tree-based method named CART will be used.

### Classification tree

A classification tree predicts that each observation belongs to the most commonly
occurring class of training observations in the region to which it belongs.

```{r tree, cache=TRUE}
tree.fit <- train(classe ~ ., method = "rpart", data = training)
tree.fit
```

Final model:

```{r plot}
fancyRpartPlot(tree.fit$finalModel)
```

Evaluation on the test set.

```{r tree.fit.test}
tree.fit.test <- predict(tree.fit, newdata = testing)
confusionMatrix(tree.fit.test, factor(testing$classe))
```

Accuracy on the test set is very close to the expected accuracy.

### Boosting

Boosting is an approach for improving prediction results from decision trees.
It is based on growing trees sequentially, using information from previously grown
trees and then combining all of the trees in order to create a single predictive
model.

```{r gbm , cache=TRUE}
gbm.fit <- train(classe ~ ., method = "gbm", data = training, verbose = FALSE)
gbm.fit
```

Evaluation on the test set.

```{r gbm.fit.test}
gbm.fit.test <- predict(gbm.fit, newdata = testing)
confusionMatrix(gbm.fit.test, factor(testing$classe))

stopCluster(cl)
```

## Conclusion

Accuracy has vastly improved from around 0.50 to 0.96 with the introduction of 
Boosting.


## Source code

Link to the source code repository:  
https://github.com/miniesta4/datasciencecoursera
